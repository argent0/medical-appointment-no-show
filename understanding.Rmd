## TODO

* <s>Split the data into a test,validation, train set</s>
* <s>Train a ctree.</s>
* <s>Use validation to adjust some parameters.</s>
* <s>Use test to evaluate the resulting model.</s>

### EDA Feedback

* Make a list of what's is to be done to get first version.
* Make better presentation of the data

### First version TODO

* Write the motivation as a first paragraph.
* Reintroduce the dataset.
* Motivate the data splitting (Why do we split? How? What have we obtained?)
* Motivate the evaluation criteria.
* Move the new EDA to the EDA post.

# Tree Analysis

This article deals with further analysing the data explored on a previous
[article](www.example.com).
Briefly, the data describes patient's appointments to the doctor. Particularly
it says whether the patient showed or not. Using the original data nomenclature:

* `No.show = Yes` means the patient missed the appointment.
* `No.show = No` means the patient showed up to the appointment.

The objective is to gain knowledge of what groups of patients are more prone to
miss their appointments. This knowledge can, in turn, be used to sustain future
policies or evaluate the effectiveness of current ones.

One way to achieve the objective is to
train a [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning).
This
is a machine learning technique that, in this case, will attempt divide the
appointments using a sequence of conditions into those that realize and those
that don't.  Although it may not be as accurate as other approaches,
decisions trees stands out for producing easy to interpret
and understand relations among the provided data and the target variable. These
produced relations will be useful tools for sustaining future decisions.

In this particular case: the **features** are data about the appointment and the patients;
while the **target variable** is whether the appointment realizes or not.

## The Resulting model

Before entering into technical details, here is the resulting tree model

**WARNING!**:

* Use <kbd>Ctrl</kbd>&nbsp;<kbd>+</kbd> to enlarge the image below.
* Use <kbd>Ctrl</kbd>&nbsp;<kbd>-</kbd> to downscale.
* Use <kbd>Ctrl</kbd>&nbsp;<kbd>0</kbd> to return to the default size.

```{r resulting_model_show ,fig.height=15, fig.width=30,cache=TRUE}
plot(treeModel)
```

The ellipses represent the nodes. Above each node there is a number inside a
square that I'll use to refer the nodes here. Below the variable name inside
the node is the estimated probability that the partition criteria is an accident
originating from random fluctuations.

For example, node 1 partitions the appointments into those that occur in the
same day (`Wait <= 0`) and those that don't (`Wait > 0`). Comparing the leaves
(Terminal nodes) to the left and to the right of node 1, it is evident that
same-day appointments have higher probability of realizing (<i>i.e.</i> to
become real).

Those patients may already be at the hospital when scheduling the appointment.
The source of the data doesn't mention whether this is the case.

Notably, among the nodes to the left of node 1 is node 6 (a leaf). Node 6 represents
same-day appointments (node 1), of patients less than 21 years old (node 2) but
older than 6 years old (node 3), who are male (node 5). The model shows that
this is the population that misses most of the same day appointments.

To right of node 1, nodes 20 and 30 show that `SMS_received > 0` increases the
chances that the patients show to their appointments. Contrary to what the
previous, more superficial, analysis suggested; sending SMSs is working for the
populations in nodes 24 and 32.

Finally, in the scope of this analysis, patients with `Scholarship` in nodes 19
and 33 are more prone to miss the appointment.

## Analysing the results.

The tree model has provided interesting insights on the structure of the data
with respect to the `No.show` variable. Yet there is further work to be done in
order to ensure that we are contemplating something more than correlation.

For example, the model shows that patients with `Scholarship > 0` are more prone
to miss the appointment. But it is also plausible that those patients are the
ones more likely to live in poverty, and thus less able to miss a workday.

Something similar could be said about `SMS_received > 0`.

Controlled experiments and statistical hypothesis testing, where applicable,
would shed light on the causality relations. The result of the tree model has
been to guide and suggest which are worthy venues for such experimentation.
Concretely:

* **The less the wait the better**: So policies that reduce the wait should be
  favoured (assuming the hospital's objective is to serve the maximum number of 
  patients).

* **SMSs seems to be working**: It still may not be the case, but experiments
  could be designed to corroborate this hypothesis. Or implement other ways of
  reminding (e.g. e-mail).

# Technical details follow

##  The Decision Tree

The tree model used in this analysing is called *conditional inference tree*.
The used implementation comes from R's
[party](https://cran.r-project.org/web/packages/party/vignettes/party.pdf)
package.

**Constraining the model to prevent overfit**

This particular analysis will constrain the tree model in two ways:

1. Each decision in the tree will have an statistical significance, of at least,
   95%. In other terms, the chance of fitting accidental features on the data is less
   than 5%.
2. The *bucket size*, the number of cases at the ends of the tree, should be
   greater than 500.

Both these constrains will improve the model's performance on unseen data.

```{r}
require("functional")
require("dplyr")
require("party")
require("Metrics")
options(knitr.package.dev=c("svg"))
options(knitr.package.echo=FALSE)
```

```{r load_data, cache=TRUE}
#Loading the data
appData <-
	read.csv(
		 unz("data/noshowappointments.zip", "KaggleV2-May-2016.csv"),
		 header=TRUE)

# Computing the wait
schedDay <- const (
	sapply(X = (appData$ScheduledDay %>% as.character %>% strsplit("T")),
	       FUN=`[[`, 1) %>%
	strptime("%Y-%m-%d") %>% as.Date)


appDay <- const (
	sapply(X = (appData$AppointmentDay %>% as.character %>% strsplit("T")),
	       FUN=`[[`, 1) %>%
	strptime("%Y-%m-%d") %>% as.Date)

appData$Wait= (appDay() - schedDay()) %>% as.numeric #in days
appData$ScheduledDay = schedDay()
appData$AppointmentDay = appDay()

# Data clean up
appData <- appData[appData$Wait>=0,]
appData <- appData[appData$Age>=0,]
```

## Some extra EDA (To be moved to the previous article)

```{r eda}
eda <- list()
eda$n_samples <- nrow(appData) #110,521
eda$n_patients <- length(unique(appData$PatientId)) #62,298
eda$n_appointments <- length(unique(appData$AppointmentID)) #one row per appointment
eda$n_neighbourhoods <- nlevels(appData$Neighbourhood) #81
eda$range_wait <- range( appData$Wait ) # Waits of up to 179 days
eda$range_scheduledDay <- range( appData$ScheduledDay ) #"2015-11-10" "2016-06-08"
eda$range_appointmentDay <- range( appData$AppointmentDay ) #"2016-04-29" "2016-06-08"
```

**Number of programmed appointments per day**

```{r appointments_per_day}
barplot(table(appData$AppointmentDay))
```

## Splitting the data into Train, Validation & Test

The splitting criterion is using by `AppointmentDay` in the following way:

* the first 14 days of data go for training
* the next 7 days for validation
* the final 6 days for testing

This results in the following sizes for each set.

```{r samples_split, cache=TRUE}
train <- appData[appData$AppointmentDay <= "2016-05-17",]
validation <- appData[appData$AppointmentDay > "2016-05-17" &
		appData$AppointmentDay <= "2016-05-31",]
test <- appData[appData$AppointmentDay > "2016-05-31",]

(function(){
	 nrows <- lapply(list(Train=train,Validation=validation,Test=test), nrow)
	 midpoints <- barplot(unlist(nrows) , ylab="Number of Rows")
	 text(midpoints,unlist(nrows)-1500,c(nrow(train),nrow(validation),nrow(test)))
	 title(main="Number of samples in each partition")
	})()

```

Unfortunately, partitioning this way doesn't distribute the maximum `Wait`
equally among the sets.
Nevertheless, this method is preferred because,
having the sets one following the other in time,
is a better portrayal of the conditions in which the model would be deployed.

```{r max_wait, cache=TRUE}
barplot(sapply(list(range(train$Wait)
		    ,range(validation$Wait)
		    ,range(test$Wait)),FUN=`[`,2)
	,names.arg=c("Train","Validation","Test")
	,ylab="Max Wait"
	,main="Max Wait in each partition")
```

## Evaluation Criteria

Log loss. Because calibrated probabilities could prove useful. Think appointment
superposition. Policies can be based on probabilities.

## Learning Curve


```{r train_tree_model, results="hide", cache=TRUE}

learning_curve_step <- function(limit_date) {
	train_set <- train$AppointmentDay <= limit_date

	validation_step <- function(tree_depth) {
		treeModel <- ctree(No.show ~ Wait + SMS_received + Age + Scholarship + Gender,
				   data=train,
				   subset=train_set,
				   control=ctree_control(mincriterion=0.95,
							 minbucket=500,
							 maxdepth=tree_depth))

		# Predict probabilities from the model
		pred_train <- unlist(lapply(treeresponse(treeModel),FUN=`[`,2))
		ground_trurh_train <- ((as.integer(train[train_set, "No.show"])+1)%%2)

		loss_train <- logLoss(ground_trurh_train, pred_train)

		pred_validation <- unlist(lapply(treeresponse(treeModel, newdata=validation),FUN=`[`,2))
		ground_trurh_validation <- ((as.integer(validation[, "No.show"])+1)%%2)
		loss_validation <- logLoss(ground_trurh_validation, pred_validation)
		return(c(loss_train, loss_validation))
	}

	result <- list(n_samples=sum(train_set),
		       loss=Vectorize(validation_step)(seq(8)))

	print(result)
	return(result)
}

learning_curve <- lapply( as.list(sort(unique(train$AppointmentDay))), learning_curve_step)
```

**Convergence of the algorithm**

An important question is *whether there is enough data to train a model*.
Complex models being, generally more data demanding than simpler ones. The
common procedure to answer this question is using a *learning curve*.

In a learning curve, increasing amounts of data is used to train the model while
observing its performance on a validation set. This is a diminishing returns
scenario where doubling the amount of data won't generally double the model's
performance. Models reach a, practically, maximum performance after which
using more data doesn't improve performance. Simpler models reach that limit
with less data than more complex ones.

This is the learning curve for this analysis:


```{r learning_curve}
best_val_perf <- sapply(learning_curve, FUN=function(elm) { return(min(elm$loss[2,])) })
best_train_perf <- sapply(learning_curve, FUN=function(elm) { return(min(elm$loss[1,])) })
best_val_tree_size <- sapply(learning_curve, FUN=function(elm) { return(which.min(elm$loss[2,])) })
best_val_train_size <- sapply(learning_curve, FUN=function(elm) { return(which.min(elm$loss[1,])) })

sample_size <- sapply(learning_curve, FUN=function(elm) {return(elm$n_samples)})
plot(sample_size, best_val_perf,ylim=range(c(best_val_perf,best_train_perf))
     ,main="Learning curve", xlab="Number of used samples",
     ylab="Performance(logloss)" )
lines(sample_size, best_val_perf,ylim=range(c(best_val_perf,best_train_perf)))
points(sample_size, best_train_perf, col=2, pch=4)
lines(sample_size, best_train_perf, col=2)
legend("bottomright", title="Performance", pch=c(4,1),
       legend=c("Traning","Validation"))
```

The performance in the validation set shows the diminishing returns of using more
data, but it seems the limit has not been reached yet after spending
all the available data. It also shows that for small number of samples, the
training and validation errors differ substantially.

One peculiarity in this analysis is that the training
error is greater than the validation error for bigger numbers of samples. As
mentioned before, this might be due to the validation set having "easier" cases
than the training set.

## Validation curve.

**Finding the ideal tree depth**

Another aspect of a tree model is its depth, the maximum number of operations
performed on the sample before getting an answer from the model.

Lower depths mean simpler to interpret models, so they are preferred. This
analysis trains trees of up to a depth of 8. The validation set is used,
afterwards, to pick the simpler one with the least error.

```{r validation_curve}
picked_training_size <- which.min(best_val_perf) #an index in the larning_curve
plot(seq(8), learning_curve[[picked_training_size]]$loss[2,]
     , xlab="Tree depth", ylab="Performance in Validation Set"
     , main="Validation curve")

picked_tree_depth = 5 # b.c there isn't much difference with 6
```

This curve suggests that a tree of depth 5 is enough for the available amount of
data.

## Evaluating the resulting model

The final model, a tree with depth 5, is trained in both validation and training
set. 

```{r resulting_model, cache=TRUE}
treeModel <- ctree(No.show ~ Wait + SMS_received + Age + Scholarship + Gender,
		   data=rbind(train, validation),
		   control=ctree_control(mincriterion=0.95,
					 minbucket=500,
					 maxdepth=picked_tree_depth))
```

```{r base_line}
pred_test <- unlist(lapply(treeresponse(treeModel, newdata=test),FUN=`[`,2))
ground_trurh_test <- ((as.integer(test[, "No.show"])+1)%%2)
ground_trurh_train <- ((as.integer(rbind(train,validation)[, "No.show"])+1)%%2)
logLoss(ground_trurh_test, pred_test)
logLoss(ground_trurh_test, mean(ground_trurh_train))
```

Its performance is evaluated using a test set. In this case, the model has
*logloss* of `0.4267598` in the test set.

**Comparison with the baseline performance**

The baseline model, in this case, in one that assigns to each patient a
probability $p$ of missing the appointment. Where $p$ is the ratio between
those who didn't showed up and the total amount of patients.

Such a basic model has a logloss of `0.4798831` in the test set, which is
greater than that of the tree model. This means that the tree model has
succeed in find populations with markedly different probabilities of showing up.

### Calibration in the test set

Another important aspect of evaluating a model's performance, for the case of
classification, is to check whether the probabilities predicted by the model are
in tune(or calibrated) with those of the population it represents.

This is evaluated using a calibration curve, where the probabilities predicted
by the model are compared against the actual ratios seen in the test set. A
reference $y=x$ line is included for reference.

```{r calibration_curve_test, cache=TRUE}
pred_test <- unlist(lapply(treeresponse(treeModel, newdata=test),FUN=`[`,2))
ground_trurh_test <- ((as.integer(test[, "No.show"])+1)%%2)

calibration_test <- lapply(split(ground_trurh_test, as.factor(pred_test)), FUN=mean)
deviation_test <- lapply(split(ground_trurh_test, as.factor(pred_test)), FUN=function(d){
				  m<- mean(d)
				  return(m*(1-m)/length(d))})

range(deviation_test)

plot(as.numeric(names(calibration_test)), calibration_test
     ,xlab="Estimated probability"
     ,ylab="Test set ratio"
     ,main="Calibration curve")
abline(0,1,lty=2)
```

The plot shows that the model generally tends to slightly underestimate the
probability of patients missing their appointments. However, this is not enough
to prevent the extraction of valuable insights from the model as the next
section shows.


## Apendix

The desviation is calculated as the posterior variance of beta bernoulli
distribution in the limit where the amount of data overhelms the prior.
